---
---

@article{10.1093/jamia/ocab029,
    abbr={JAMIA},
    author = {Rodriguez, Victor Alfonso <sup>*</sup> and <b>Bhave, Shreyas <sup>*</sup></b> and Chen, Ruijun and Pang, Chao and Hripcsak, George and Sengupta, Soumitra and Elhadad, Noemie and Green, Robert and Adelman, Jason and Metitiri, Katherine Schlosser and Elias, Pierre and Groves, Holden and Mohan, Sumit and Natarajan, Karthik and Perotte, Adler},
    title = "{Development and validation of prediction models for mechanical ventilation, renal replacement therapy, and readmission in COVID-19 patients}",
    journal = {Journal of the American Medical Informatics Association},
    year = {2021},
    month = {03},
    abstract = "{Coronavirus disease 2019 (COVID-19) patients are at risk for resource-intensive outcomes including mechanical ventilation (MV), renal replacement therapy (RRT), and readmission. Accurate outcome prognostication could facilitate hospital resource allocation. We develop and validate predictive models for each outcome using retrospective electronic health record data for COVID-19 patients treated between March 2 and May 6, 2020.For each outcome, we trained 3 classes of prediction models using clinical data for a cohort of SARS-CoV-2 (severe acute respiratory syndrome coronavirus 2)–positive patients (n = 2256). Cross-validation was used to select the best-performing models per the areas under the receiver-operating characteristic and precision-recall curves. Models were validated using a held-out cohort (n = 855). We measured each model’s calibration and evaluated feature importances to interpret model output.The predictive performance for our selected models on the held-out cohort was as follows: area under the receiver-operating characteristic curve—MV 0.743 (95\\% CI, 0.682-0.812), RRT 0.847 (95\\% CI, 0.772-0.936), readmission 0.871 (95\\% CI, 0.830-0.917); area under the precision-recall curve—MV 0.137 (95\\% CI, 0.047-0.175), RRT 0.325 (95\\% CI, 0.117-0.497), readmission 0.504 (95\\% CI, 0.388-0.604). Predictions were well calibrated, and the most important features within each model were consistent with clinical intuition.Our models produce performant, well-calibrated, and interpretable predictions for COVID-19 patients at risk for the target outcomes. They demonstrate the potential to accurately estimate outcome prognosis in resource-constrained care sites managing COVID-19 patients.We develop and validate prognostic models targeting MV, RRT, and readmission for hospitalized COVID-19 patients which produce accurate, interpretable predictions. Additional external validation studies are needed to further verify the generalizability of our results.}",
    issn = {1527-974X},
    doi = {10.1093/jamia/ocab029},
    html = {https://doi.org/10.1093/jamia/ocab029},
    note = {ocab029},
    eprint = {https://academic.oup.com/jamia/advance-article-pdf/doi/10.1093/jamia/ocab029/36578465/ocab029.pdf},
    selected={true} 
}

@InProceedings{pmlr-v136-adams20a,
  abbr =         {ML4H@NeurIPS},
  title = 	 {Zero-Shot Clinical Acronym Expansion via Latent Meaning Cells},
  author =       {Adams, Griffin and Ketenci, Mert and <b>Bhave, Shreyas</b> and Perotte, Adler and Elhadad, No\'emie},
  booktitle = 	 {Proceedings of the Machine Learning for Health NeurIPS Workshop},
  pages = 	 {12--40},
  year = 	 {2020},
  editor = 	 {Emily Alsentzer and Matthew B. A. McDermott and Fabian Falck and Suproteem K. Sarkar and Subhrajit Roy and Stephanie L. Hyland},
  volume = 	 {136},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {11 Dec},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v136/adams20a/adams20a.pdf},
  html = 	 {http://proceedings.mlr.press/v136/adams20a.html},
  abstract = 	 {We introduce Latent Meaning Cells, a deep latent variable model which learns contextualized representations of words by combining local lexical context and metadata. Metadata can refer to granular context, such as section type, or to more global context, such as unique document ids. Reliance on metadata for contextualized representation learning is apropos in the clinical domain where text is semi-structured and expresses high variation in topics. We evaluate the LMC model on the task of zero-shot clinical acronym expansion across three datasets. The LMC significantly outperforms a diverse set of baselines at a fraction of the pre-training cost and learns clinically coherent representations. We demonstrate that not only is metadata itself very helpful for the task, but that the LMC inference algorithm provides an additional large benefit.},
  selected={true} 
}

@article{ artemiss2020,
abbr={ARTEMISS@ICML},
title={Information Theoretic Approaches for Testing Missingness in Predictive Models},
author={<b>Bhave, Shreyas</b> and Ranganath, Rajesh and Perotte, Adler},
journal={ARTEMISS Workshop, International Conference on Machine Learning},
year={2020},
html={https://openreview.net/forum?id=6Y05VJfGlFM},
pdf={https://openreview.net/pdf?id=6Y05VJfGlFM},
selected={true} 
}